{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steering_Akhil_Sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjtpAf9kMxu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.utils.tensorboard as tb\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch import save\n",
        "from torch import load\n",
        "from os import path\n",
        "import torchvision.models as models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from torch import save\n",
        "from torch import load\n",
        "from os import path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo-FsC5hM615",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD81Hr71NIh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set current directory\n",
        "\n",
        "%cd /content/drive/My Drive/Steering_Datasets_Rev1\n",
        "!unzip CH2_002_Test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAMZsnkaSicC",
        "colab_type": "text"
      },
      "source": [
        "# DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvxQFobf9Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SteeringDataset(Dataset):\n",
        "    def __init__(self, image_path, data_transforms=None):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Hint: Use the python csv library to parse labels.csv\n",
        "        \"\"\"\n",
        "        # self.name_label_path = name_label_path\n",
        "        self.image_path = image_path\n",
        "        self.name = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(5,6))\n",
        "\n",
        "        self.labels = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(6,7))\n",
        "        self.center_data = pd.concat([self.name, self.labels], axis=1) #combine image name and label dataframes\n",
        "        self.center_data = self.center_data[self.center_data[\"filename\"].str.contains('center')] # only keep center image names and labels\n",
        "\n",
        "        self.name = pd.DataFrame(self.center_data[self.center_data.columns[0]]) # center images names\n",
        "        self.len = self.name.shape[0]\n",
        "        self.labels = pd.DataFrame(self.center_data[self.center_data.columns[1]]) # center image labels\n",
        "\n",
        "        # bin angles into 100 classes\n",
        "\n",
        "        # classes = 100\n",
        "        # bins = np.linspace(-2.1,2,classes)\n",
        "        # labels = np.linspace(1,classes-1,classes-1).astype(int)\n",
        "        # self.labels = pd.cut(self.labels['angle'], bins = bins, labels = labels)\n",
        "\n",
        "        print(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        return a tuple: img, label\n",
        "        \"\"\"\n",
        "        # img = Image.open(str(self.name.iloc[idx, 0]))\n",
        "        img = Image.open(str(self.name.iloc[idx, 0])[7:]) # [7:] removes 'center/' before the image number since the zipped images just have numbers\n",
        "        img = img.resize((320,240)) #resize image\n",
        "        transform = transforms.ToTensor()\n",
        "        img = transform(img)\n",
        "        label = self.labels.iloc[idx][0]\n",
        "        return img, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBI0gUIqZwmD",
        "colab_type": "text"
      },
      "source": [
        "# Save and Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpcbo4cZumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset_path, data_transforms, num_workers, batch_size, shuffle):\n",
        "    dataset = SteeringDataset(dataset_path, data_transforms)\n",
        "    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def train_test_split(args, dataset):\n",
        "    # Indices of Split\n",
        "    size = len(dataset)\n",
        "    indices = list(range(size))\n",
        "    train_size = int(args.test_fraction * size)\n",
        "    train_indices, test_indices = indices[train_size:], indices[:train_size]\n",
        "\n",
        "    # SubsetRandomSamplers\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # DataLoaders\n",
        "    train = DataLoader(dataset, batch_size=args.train_batch_size, sampler=train_sampler)\n",
        "    test = DataLoader(dataset, batch_size=args.test_batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def save_model(model, file_name):\n",
        "    #if isinstance(model, CNNClassifier):\n",
        "    return save(model.state_dict(), path.join(path.abspath(''), file_name))\n",
        "\n",
        "    #raise ValueError(\"model type '%s' not supported!\"%str(type(model)))\n",
        "\n",
        "def load_model(file_name):\n",
        "    r = CustomModel()\n",
        "    r.load_state_dict(load(path.join(path.abspath(''), file_name), map_location='cpu'))\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptEBrTX3YLl9",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1R0Fxl7YHMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationLoss(torch.nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Compute mean(-log(softmax(input)_label))\n",
        "        @input:  torch.Tensor((B,C)), where B = batch size, C = number of classes\n",
        "        @target: torch.Tensor((B,), dtype=torch.int64)\n",
        "        @return:  torch.Tensor((,))\n",
        "        Hint: Don't be too fancy, this is a one-liner\n",
        "        \"\"\"\n",
        "        m = nn.MSELoss()\n",
        "        return torch.sqrt(m(input.view(input.size(0)),target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbPyUoNYlSC",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5gGmDXZFhW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 5, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(5, 15, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(67860, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        @x: torch.Tensor((B,3,64,64))\n",
        "        @return: torch.Tensor((B,6))\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMRS_itGYoWS",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ulUussYd_f",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6F2zxvjYCVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "\n",
        "class Args(object):\n",
        "    def __init__(self, model):\n",
        "\n",
        "        # load_data\n",
        "        self.data_path = ''\n",
        "        self.transforms = None  # not in use\n",
        "        self.num_workers = 0  # not in use\n",
        "        self.train_batch_size = 200\n",
        "        self.test_batch_size = 1  # b/c batches aren't needed for testing\n",
        "        self.shuffle = False  # not in use\n",
        "\n",
        "        #train_test_split\n",
        "        self.test_fraction = 0.2\n",
        "\n",
        "        # Device\n",
        "        self.device = torch.device(\"cuda\")\n",
        "\n",
        "        # train\n",
        "        self.learning_rate = 0.01 \n",
        "        self.momentum = 0.9 # not in use \n",
        "        self.epochs = 10\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = ClassificationLoss() #nn.CrossEntropyLoss() #RMSELoss().   CrossEntropyLoss: lable.long() conversion needed in train()\n",
        "\n",
        "        # test\n",
        "        self.correct_threshold = 0.1  # compared with prediction loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJMppDDJYqJT",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysIS-u6Fm_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args,model,train_data, test_data):\n",
        "    \"\"\"\n",
        "    Your code here\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize weights\n",
        "\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, mean=0, std=0.1)\n",
        "            torch.nn.init.constant_(m.bias, 0.1)\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            torch.nn.init.normal_(m.weight, mean=0, std=0.1)\n",
        "\n",
        "\n",
        "    # Model\n",
        "    model.train()\n",
        "    model.to(args.device)\n",
        "    \n",
        "    # Optimizer & Criterion\n",
        "    optimizer = args.optimizer\n",
        "    criterion = args.criterion\n",
        "    criterion.to(args.device)\n",
        "\n",
        "\n",
        "    # Book Keeping\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "    num_steps_per_epoch = len([i for i in range(len(train_data)) if i%10==0])\n",
        "\n",
        "\n",
        "    # Epoch Loop\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        stepper = 0\n",
        "        model.train()\n",
        "        \n",
        "        # Batch Loop\n",
        "        \n",
        "        \n",
        "        for batch_idx, (data, label) in enumerate(train_data):\n",
        "            \n",
        "            print('epoch: ', epoch, '.  batch_idx: ', batch_idx)\n",
        "\n",
        "\n",
        "            data, label = data.to(args.device), label.to(args.device)\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            \n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Printing & Logging\n",
        "            if batch_idx % 1 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(train_data.dataset),\n",
        "                  100. * batch_idx / len(train_data), loss.item()))                \n",
        "                train_losses.append(loss.item())\n",
        "                train_counter.append((batch_idx*64) + ((epoch-1)*len(train_data.dataset)))\n",
        "                stepper += 1\n",
        "\n",
        "        # check validation loss every epoch\n",
        "        \n",
        "        valid_loss = test(args, model, test_data)\n",
        "        print(\"validation loss = \", valid_loss)\n",
        "\n",
        "    save_model(model,'cnn_akhil')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdXYzaoKYuuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, test_data):\n",
        "\n",
        "    # Model\n",
        "    model.eval()\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    # Loss\n",
        "    criterion = args.criterion\n",
        "    criterion.to(args.device)\n",
        "\n",
        "    # Book Keeping\n",
        "    test_loss = 0\n",
        "    correct = 0  # Since no classes: correct++ occurs when prediction is sufficiently close to target (within a threshold)\n",
        "    test_progress = 0\n",
        "\n",
        "    # Data Loop\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_data:\n",
        "\n",
        "            # Use Model\n",
        "            data, target = data.to(args.device), target.to(args.device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target).item()\n",
        "            test_loss += loss\n",
        "\n",
        "        test_loss /= len(test_data)\n",
        "\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qYyqhlJRBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "\n",
        "model = CNNClassifier()\n",
        "\n",
        "# Parameters\n",
        "args = Args(model)\n",
        "\n",
        "# Load Data & Train/Test Split\n",
        "print('Loading Data...')\n",
        "train_data, test_data = train_test_split(args, SteeringDataset(args.data_path, args.transforms))\n",
        "\n",
        "print('Training...')\n",
        "train(args, model, train_data, test_data)\n",
        "\n",
        "# Test Model\n",
        "test(args, model, test_data) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}