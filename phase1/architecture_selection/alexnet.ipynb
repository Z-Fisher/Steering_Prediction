{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_anthony.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LteqG5rbTVqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/Steering_Datasets_Rev1 (1)/center_only.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4X3HQuRjwc",
        "colab_type": "text"
      },
      "source": [
        "## Dataloader for training data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIRw9R9yRmev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torchvision import transforms, utils \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd \n",
        "\n",
        "class SteeringDataset(Dataset):\n",
        "    def __init__(self, image_path, data_transforms=None):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Hint: Use the python csv library to parse labels.csv\n",
        "        \"\"\"\n",
        "        # self.name_label_path = name_label_path\n",
        "        self.image_path = image_path\n",
        "        self.name = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(5,6))\n",
        "\n",
        "        self.labels = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(6,7))\n",
        "        self.center_data = pd.concat([self.name, self.labels], axis=1) #combine image name and label dataframes\n",
        "        self.center_data = self.center_data[self.center_data[\"filename\"].str.contains('center')] # only keep center image names and labels\n",
        "\n",
        "        self.name = pd.DataFrame(self.center_data[self.center_data.columns[0]]) # center images names\n",
        "        self.len = self.name.shape[0]\n",
        "        self.labels = pd.DataFrame(self.center_data[self.center_data.columns[1]]) # center image labels\n",
        "\n",
        "        # bin angles into 100 classes\n",
        "\n",
        "        # classes = 100\n",
        "        # bins = np.linspace(-2.1,2,classes)\n",
        "        # labels = np.linspace(1,classes-1,classes-1).astype(int)\n",
        "        # self.labels = pd.cut(self.labels['angle'], bins = bins, labels = labels)\n",
        "\n",
        "        # print(self.name)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        return a tuple: img, label\n",
        "        \"\"\"\n",
        "        # img = Image.open(str(self.name.iloc[idx, 0]))\n",
        "        img = Image.open(self.image_path + str(self.name.iloc[idx, 0])[7:]) # [7:] removes 'center/' before the image number since the zipped images just have numbers\n",
        "        img = img.crop((0, 240, 640, 480))  # crop image (remove above horizon). Original dimensions: 640,480. changing to 640,240    \n",
        "        img = img.resize((224,224)) #resize image --> pretrained alexnet model needs img sizes of 224 x 224\n",
        "        transform = transforms.ToTensor()\n",
        "        img = transform(img)\n",
        "        label = self.labels.iloc[idx][0]\n",
        "        return (img, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScZOnp2YM4n",
        "colab_type": "text"
      },
      "source": [
        "## Dataloader for test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92BUohWqXyH_",
        "colab_type": "code",
        "outputId": "f090dd3d-3b3f-480c-af18-e3cc137a3895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch \n",
        "import torchvision \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "from torchvision import transforms \n",
        "\n",
        "from PIL import Image \n",
        "import numpy as np \n",
        "import os \n",
        "import pandas as pd \n",
        "\n",
        "class TestSet(Dataset): \n",
        "  def __init__(self, image_path, data_transform): \n",
        "    self.image_path = image_path \n",
        "    self.data_transform = data_transform \n",
        "\n",
        "    self.dataset = pd.read_csv('/content/drive/My Drive/Steering_Datasets_Rev1 (1)/CH2_002_Test/steering.csv') \n",
        "    print(type(self.dataset.iloc[0,0]))\n",
        "  def __len__(self): \n",
        "    return len(self.dataset) \n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    img_name = os.path.join(self.image_path, str(self.dataset.iloc[idx,0])+'.jpg')\n",
        "    img = Image.open(img_name)\n",
        "    img = img.crop((0,240,640,480))\n",
        "    img = img.resize((224,224))\n",
        "    if self.data_transform: \n",
        "      img = self.data_transform(img)\n",
        "\n",
        "    label = self.dataset.iloc[idx,1] \n",
        "\n",
        "    item = (img,label)\n",
        "    return item \n",
        "\n",
        "\n",
        "## test to see if this works \n",
        "img_path = '/content/drive/My Drive/Steering_Datasets_Rev1 (1)/CH2_002_Test/center/'\n",
        "test_data = TestSet(image_path=img_path, data_transform=transforms.ToTensor()) \n",
        "print(len(test_data)) \n",
        "# for idx, (data, label) in enumerate(test_data): \n",
        "#   print(f'idx: {idx},   data: {data.shape},   label:{label}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.int64'>\n",
            "5614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw3sWRooEUxh",
        "colab_type": "text"
      },
      "source": [
        "## Loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNjmCHAGEXAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn \n",
        "\n",
        "class ClassificationLoss(torch.nn.Module): \n",
        "  def forward(self,input,target):\n",
        "    m = nn.MSELoss() \n",
        "    return torch.sqrt(m(input.view(input.size(0)),target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s513p5OSO4d",
        "colab_type": "text"
      },
      "source": [
        "## Train loader and Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBzGExp-SR-3",
        "colab_type": "code",
        "outputId": "5f9a8be3-a782-40ac-b6d2-8112db646ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "################################# LOADER ######################################################################\n",
        "train_set = SteeringDataset(image_path='/content/center_only/', data_transforms=transforms.ToTensor())\n",
        "for idx, (data, label) in enumerate(train_set): \n",
        "  print(f'Image size: {data.shape}') \n",
        "  if idx == 0: \n",
        "    break  \n",
        "\n",
        "B = 16\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=B, shuffle=True)\n",
        "\n",
        "#################################### MODEL #####################################################################\n",
        "import torchvision.models as models \n",
        "\n",
        "alexnet = models.alexnet(pretrained=False, progress=True)\n",
        "for param in alexnet.parameters(): \n",
        "  param.requires_grad = True \n",
        "alexnet.classifier[6] = nn.Linear(4096,1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size: torch.Size([3, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BLmWcZTUgL1",
        "colab_type": "text"
      },
      "source": [
        "## Train on Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onUKuj9NUhxR",
        "colab_type": "code",
        "outputId": "8760f87f-1b9e-4db8-cef9-954dc408ee57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class Args(): \n",
        "  def __init__(self): \n",
        "    self.num_epochs = 2 \n",
        "    self.learning_rate = 0.001 \n",
        "    self.log_interval = 20 \n",
        "\n",
        "args = Args() \n",
        "\n",
        "import torch.optim as optim \n",
        "def train(args, model, train_loader): \n",
        "  model.train() \n",
        "\n",
        "  # define optimizer and loss \n",
        "  optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) \n",
        "  criterion = ClassificationLoss() \n",
        "\n",
        "  train_losses = [] \n",
        "  loss_list = [] \n",
        "  for epoch in range(1,args.num_epochs+1):\n",
        "    ### \n",
        "    for batch_idx, (data, target) in enumerate(train_loader): \n",
        "      optimizer.zero_grad() \n",
        "      output = model(data)\n",
        "      loss = criterion(output,target)\n",
        "      loss.backward() \n",
        "      optimizer.step() \n",
        "\n",
        "      loss_list.append(loss.item()) \n",
        "      if batch_idx % args.log_interval == 0: \n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, (batch_idx+1) * len(data), len(train_loader)*len(data),\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "        torch.save(model.state_dict(), './pretrained_alexnet.pt')\n",
        "    print(f'\\n Average loss for Epoch {epoch}: {sum(loss_list)/len(loss_list)} \\n')\n",
        "    del loss_list[:]\n",
        "\n",
        "\n",
        "train(args, alexnet, train_loader) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [16/33808 (0%)]\tLoss: 0.280956\n",
            "Train Epoch: 1 [336/33808 (1%)]\tLoss: 0.346788\n",
            "Train Epoch: 1 [656/33808 (2%)]\tLoss: 0.360874\n",
            "Train Epoch: 1 [976/33808 (3%)]\tLoss: 0.213135\n",
            "Train Epoch: 1 [1296/33808 (4%)]\tLoss: 0.462182\n",
            "Train Epoch: 1 [1616/33808 (5%)]\tLoss: 0.176479\n",
            "Train Epoch: 1 [1936/33808 (6%)]\tLoss: 0.179577\n",
            "Train Epoch: 1 [2256/33808 (7%)]\tLoss: 0.283545\n",
            "Train Epoch: 1 [2576/33808 (8%)]\tLoss: 0.356300\n",
            "Train Epoch: 1 [2896/33808 (9%)]\tLoss: 0.233821\n",
            "Train Epoch: 1 [3216/33808 (9%)]\tLoss: 0.388968\n",
            "Train Epoch: 1 [3536/33808 (10%)]\tLoss: 0.321891\n",
            "Train Epoch: 1 [3856/33808 (11%)]\tLoss: 0.139951\n",
            "Train Epoch: 1 [4176/33808 (12%)]\tLoss: 0.302057\n",
            "Train Epoch: 1 [4496/33808 (13%)]\tLoss: 0.232436\n",
            "Train Epoch: 1 [4816/33808 (14%)]\tLoss: 0.276607\n",
            "Train Epoch: 1 [5136/33808 (15%)]\tLoss: 0.164164\n",
            "Train Epoch: 1 [5456/33808 (16%)]\tLoss: 0.298527\n",
            "Train Epoch: 1 [5776/33808 (17%)]\tLoss: 0.180726\n",
            "Train Epoch: 1 [6096/33808 (18%)]\tLoss: 0.355537\n",
            "Train Epoch: 1 [6416/33808 (19%)]\tLoss: 0.189919\n",
            "Train Epoch: 1 [6736/33808 (20%)]\tLoss: 0.452787\n",
            "Train Epoch: 1 [7056/33808 (21%)]\tLoss: 0.198495\n",
            "Train Epoch: 1 [7376/33808 (22%)]\tLoss: 0.171074\n",
            "Train Epoch: 1 [7696/33808 (23%)]\tLoss: 0.253681\n",
            "Train Epoch: 1 [8016/33808 (24%)]\tLoss: 0.229612\n",
            "Train Epoch: 1 [8336/33808 (25%)]\tLoss: 0.139559\n",
            "Train Epoch: 1 [8656/33808 (26%)]\tLoss: 0.152749\n",
            "Train Epoch: 1 [8976/33808 (27%)]\tLoss: 0.398107\n",
            "Train Epoch: 1 [9296/33808 (27%)]\tLoss: 0.302068\n",
            "Train Epoch: 1 [9616/33808 (28%)]\tLoss: 0.268671\n",
            "Train Epoch: 1 [9936/33808 (29%)]\tLoss: 0.165183\n",
            "Train Epoch: 1 [10256/33808 (30%)]\tLoss: 0.203642\n",
            "Train Epoch: 1 [10576/33808 (31%)]\tLoss: 0.292813\n",
            "Train Epoch: 1 [10896/33808 (32%)]\tLoss: 0.111770\n",
            "Train Epoch: 1 [11216/33808 (33%)]\tLoss: 0.434644\n",
            "Train Epoch: 1 [11536/33808 (34%)]\tLoss: 0.209342\n",
            "Train Epoch: 1 [11856/33808 (35%)]\tLoss: 0.408020\n",
            "Train Epoch: 1 [12176/33808 (36%)]\tLoss: 0.363671\n",
            "Train Epoch: 1 [12496/33808 (37%)]\tLoss: 0.731711\n",
            "Train Epoch: 1 [12816/33808 (38%)]\tLoss: 0.330911\n",
            "Train Epoch: 1 [13136/33808 (39%)]\tLoss: 0.150732\n",
            "Train Epoch: 1 [13456/33808 (40%)]\tLoss: 0.311960\n",
            "Train Epoch: 1 [13776/33808 (41%)]\tLoss: 0.330864\n",
            "Train Epoch: 1 [14096/33808 (42%)]\tLoss: 0.173812\n",
            "Train Epoch: 1 [14416/33808 (43%)]\tLoss: 0.299311\n",
            "Train Epoch: 1 [14736/33808 (44%)]\tLoss: 0.273156\n",
            "Train Epoch: 1 [15056/33808 (44%)]\tLoss: 0.162009\n",
            "Train Epoch: 1 [15376/33808 (45%)]\tLoss: 0.426443\n",
            "Train Epoch: 1 [15696/33808 (46%)]\tLoss: 0.083276\n",
            "Train Epoch: 1 [16016/33808 (47%)]\tLoss: 0.546316\n",
            "Train Epoch: 1 [16336/33808 (48%)]\tLoss: 0.238194\n",
            "Train Epoch: 1 [16656/33808 (49%)]\tLoss: 0.266310\n",
            "Train Epoch: 1 [16976/33808 (50%)]\tLoss: 0.210106\n",
            "Train Epoch: 1 [17296/33808 (51%)]\tLoss: 0.146270\n",
            "Train Epoch: 1 [17616/33808 (52%)]\tLoss: 0.241971\n",
            "Train Epoch: 1 [17936/33808 (53%)]\tLoss: 0.175645\n",
            "Train Epoch: 1 [18256/33808 (54%)]\tLoss: 0.123141\n",
            "Train Epoch: 1 [18576/33808 (55%)]\tLoss: 0.187419\n",
            "Train Epoch: 1 [18896/33808 (56%)]\tLoss: 0.276550\n",
            "Train Epoch: 1 [19216/33808 (57%)]\tLoss: 0.451979\n",
            "Train Epoch: 1 [19536/33808 (58%)]\tLoss: 0.489166\n",
            "Train Epoch: 1 [19856/33808 (59%)]\tLoss: 0.530097\n",
            "Train Epoch: 1 [20176/33808 (60%)]\tLoss: 0.305718\n",
            "Train Epoch: 1 [20496/33808 (61%)]\tLoss: 0.230612\n",
            "Train Epoch: 1 [20816/33808 (62%)]\tLoss: 0.315437\n",
            "Train Epoch: 1 [21136/33808 (62%)]\tLoss: 0.215237\n",
            "Train Epoch: 1 [21456/33808 (63%)]\tLoss: 0.239804\n",
            "Train Epoch: 1 [21776/33808 (64%)]\tLoss: 0.173195\n",
            "Train Epoch: 1 [22096/33808 (65%)]\tLoss: 0.513786\n",
            "Train Epoch: 1 [22416/33808 (66%)]\tLoss: 0.130206\n",
            "Train Epoch: 1 [22736/33808 (67%)]\tLoss: 0.333694\n",
            "Train Epoch: 1 [23056/33808 (68%)]\tLoss: 0.188374\n",
            "Train Epoch: 1 [23376/33808 (69%)]\tLoss: 0.093185\n",
            "Train Epoch: 1 [23696/33808 (70%)]\tLoss: 0.299712\n",
            "Train Epoch: 1 [24016/33808 (71%)]\tLoss: 0.146551\n",
            "Train Epoch: 1 [24336/33808 (72%)]\tLoss: 0.161328\n",
            "Train Epoch: 1 [24656/33808 (73%)]\tLoss: 0.159678\n",
            "Train Epoch: 1 [24976/33808 (74%)]\tLoss: 0.227020\n",
            "Train Epoch: 1 [25296/33808 (75%)]\tLoss: 0.175874\n",
            "Train Epoch: 1 [25616/33808 (76%)]\tLoss: 0.137826\n",
            "Train Epoch: 1 [25936/33808 (77%)]\tLoss: 0.183734\n",
            "Train Epoch: 1 [26256/33808 (78%)]\tLoss: 0.219196\n",
            "Train Epoch: 1 [26576/33808 (79%)]\tLoss: 0.100697\n",
            "Train Epoch: 1 [26896/33808 (80%)]\tLoss: 0.198777\n",
            "Train Epoch: 1 [27216/33808 (80%)]\tLoss: 0.327723\n",
            "Train Epoch: 1 [27536/33808 (81%)]\tLoss: 0.163743\n",
            "Train Epoch: 1 [27856/33808 (82%)]\tLoss: 0.164567\n",
            "Train Epoch: 1 [28176/33808 (83%)]\tLoss: 0.299483\n",
            "Train Epoch: 1 [28496/33808 (84%)]\tLoss: 0.308233\n",
            "Train Epoch: 1 [28816/33808 (85%)]\tLoss: 0.201627\n",
            "Train Epoch: 1 [29136/33808 (86%)]\tLoss: 0.275592\n",
            "Train Epoch: 1 [29456/33808 (87%)]\tLoss: 0.214884\n",
            "Train Epoch: 1 [29776/33808 (88%)]\tLoss: 0.276299\n",
            "Train Epoch: 1 [30096/33808 (89%)]\tLoss: 0.301770\n",
            "Train Epoch: 1 [30416/33808 (90%)]\tLoss: 0.403859\n",
            "Train Epoch: 1 [30736/33808 (91%)]\tLoss: 0.145411\n",
            "Train Epoch: 1 [31056/33808 (92%)]\tLoss: 0.286989\n",
            "Train Epoch: 1 [31376/33808 (93%)]\tLoss: 0.356665\n",
            "Train Epoch: 1 [31696/33808 (94%)]\tLoss: 0.193631\n",
            "Train Epoch: 1 [32016/33808 (95%)]\tLoss: 0.134072\n",
            "Train Epoch: 1 [32336/33808 (96%)]\tLoss: 0.184631\n",
            "Train Epoch: 1 [32656/33808 (97%)]\tLoss: 0.641590\n",
            "Train Epoch: 1 [32976/33808 (97%)]\tLoss: 0.332494\n",
            "Train Epoch: 1 [33296/33808 (98%)]\tLoss: 0.191050\n",
            "Train Epoch: 1 [33616/33808 (99%)]\tLoss: 0.199011\n",
            "\n",
            " Average loss for Epoch 1: 0.24945808404248246 \n",
            "\n",
            "Train Epoch: 2 [16/33808 (0%)]\tLoss: 0.131525\n",
            "Train Epoch: 2 [336/33808 (1%)]\tLoss: 0.328818\n",
            "Train Epoch: 2 [656/33808 (2%)]\tLoss: 0.223829\n",
            "Train Epoch: 2 [976/33808 (3%)]\tLoss: 0.108433\n",
            "Train Epoch: 2 [1296/33808 (4%)]\tLoss: 0.206783\n",
            "Train Epoch: 2 [1616/33808 (5%)]\tLoss: 0.339232\n",
            "Train Epoch: 2 [1936/33808 (6%)]\tLoss: 0.231640\n",
            "Train Epoch: 2 [2256/33808 (7%)]\tLoss: 0.160065\n",
            "Train Epoch: 2 [2576/33808 (8%)]\tLoss: 0.172396\n",
            "Train Epoch: 2 [2896/33808 (9%)]\tLoss: 0.305706\n",
            "Train Epoch: 2 [3216/33808 (9%)]\tLoss: 0.429697\n",
            "Train Epoch: 2 [3536/33808 (10%)]\tLoss: 0.258688\n",
            "Train Epoch: 2 [3856/33808 (11%)]\tLoss: 0.526175\n",
            "Train Epoch: 2 [4176/33808 (12%)]\tLoss: 0.124032\n",
            "Train Epoch: 2 [4496/33808 (13%)]\tLoss: 0.431883\n",
            "Train Epoch: 2 [4816/33808 (14%)]\tLoss: 0.261498\n",
            "Train Epoch: 2 [5136/33808 (15%)]\tLoss: 0.260602\n",
            "Train Epoch: 2 [5456/33808 (16%)]\tLoss: 0.137678\n",
            "Train Epoch: 2 [5776/33808 (17%)]\tLoss: 0.150272\n",
            "Train Epoch: 2 [6096/33808 (18%)]\tLoss: 0.318050\n",
            "Train Epoch: 2 [6416/33808 (19%)]\tLoss: 0.443074\n",
            "Train Epoch: 2 [6736/33808 (20%)]\tLoss: 0.241175\n",
            "Train Epoch: 2 [7056/33808 (21%)]\tLoss: 0.257152\n",
            "Train Epoch: 2 [7376/33808 (22%)]\tLoss: 0.167156\n",
            "Train Epoch: 2 [7696/33808 (23%)]\tLoss: 0.211346\n",
            "Train Epoch: 2 [8016/33808 (24%)]\tLoss: 0.118221\n",
            "Train Epoch: 2 [8336/33808 (25%)]\tLoss: 0.223448\n",
            "Train Epoch: 2 [8656/33808 (26%)]\tLoss: 0.193142\n",
            "Train Epoch: 2 [8976/33808 (27%)]\tLoss: 0.083670\n",
            "Train Epoch: 2 [9296/33808 (27%)]\tLoss: 0.078537\n",
            "Train Epoch: 2 [9616/33808 (28%)]\tLoss: 0.219136\n",
            "Train Epoch: 2 [9936/33808 (29%)]\tLoss: 0.231771\n",
            "Train Epoch: 2 [10256/33808 (30%)]\tLoss: 0.174067\n",
            "Train Epoch: 2 [10576/33808 (31%)]\tLoss: 0.094945\n",
            "Train Epoch: 2 [10896/33808 (32%)]\tLoss: 0.182660\n",
            "Train Epoch: 2 [11216/33808 (33%)]\tLoss: 0.223563\n",
            "Train Epoch: 2 [11536/33808 (34%)]\tLoss: 0.151561\n",
            "Train Epoch: 2 [11856/33808 (35%)]\tLoss: 0.212244\n",
            "Train Epoch: 2 [12176/33808 (36%)]\tLoss: 0.098977\n",
            "Train Epoch: 2 [12496/33808 (37%)]\tLoss: 0.538697\n",
            "Train Epoch: 2 [12816/33808 (38%)]\tLoss: 0.132422\n",
            "Train Epoch: 2 [13136/33808 (39%)]\tLoss: 0.217258\n",
            "Train Epoch: 2 [13456/33808 (40%)]\tLoss: 0.193772\n",
            "Train Epoch: 2 [13776/33808 (41%)]\tLoss: 0.125186\n",
            "Train Epoch: 2 [14096/33808 (42%)]\tLoss: 0.334209\n",
            "Train Epoch: 2 [14416/33808 (43%)]\tLoss: 0.348593\n",
            "Train Epoch: 2 [14736/33808 (44%)]\tLoss: 0.319611\n",
            "Train Epoch: 2 [15056/33808 (44%)]\tLoss: 0.509036\n",
            "Train Epoch: 2 [15376/33808 (45%)]\tLoss: 0.244457\n",
            "Train Epoch: 2 [15696/33808 (46%)]\tLoss: 0.564066\n",
            "Train Epoch: 2 [16016/33808 (47%)]\tLoss: 0.156104\n",
            "Train Epoch: 2 [16336/33808 (48%)]\tLoss: 0.199959\n",
            "Train Epoch: 2 [16656/33808 (49%)]\tLoss: 0.256393\n",
            "Train Epoch: 2 [16976/33808 (50%)]\tLoss: 0.512940\n",
            "Train Epoch: 2 [17296/33808 (51%)]\tLoss: 0.459836\n",
            "Train Epoch: 2 [17616/33808 (52%)]\tLoss: 0.259012\n",
            "Train Epoch: 2 [17936/33808 (53%)]\tLoss: 0.120632\n",
            "Train Epoch: 2 [18256/33808 (54%)]\tLoss: 0.286071\n",
            "Train Epoch: 2 [18576/33808 (55%)]\tLoss: 0.158063\n",
            "Train Epoch: 2 [18896/33808 (56%)]\tLoss: 0.179014\n",
            "Train Epoch: 2 [19216/33808 (57%)]\tLoss: 0.092109\n",
            "Train Epoch: 2 [19536/33808 (58%)]\tLoss: 0.319190\n",
            "Train Epoch: 2 [19856/33808 (59%)]\tLoss: 0.383768\n",
            "Train Epoch: 2 [20176/33808 (60%)]\tLoss: 0.323009\n",
            "Train Epoch: 2 [20496/33808 (61%)]\tLoss: 0.179699\n",
            "Train Epoch: 2 [20816/33808 (62%)]\tLoss: 0.110269\n",
            "Train Epoch: 2 [21136/33808 (62%)]\tLoss: 0.382185\n",
            "Train Epoch: 2 [21456/33808 (63%)]\tLoss: 0.349229\n",
            "Train Epoch: 2 [21776/33808 (64%)]\tLoss: 0.388689\n",
            "Train Epoch: 2 [22096/33808 (65%)]\tLoss: 0.411954\n",
            "Train Epoch: 2 [22416/33808 (66%)]\tLoss: 0.259094\n",
            "Train Epoch: 2 [22736/33808 (67%)]\tLoss: 0.183287\n",
            "Train Epoch: 2 [23056/33808 (68%)]\tLoss: 0.297262\n",
            "Train Epoch: 2 [23376/33808 (69%)]\tLoss: 0.470606\n",
            "Train Epoch: 2 [23696/33808 (70%)]\tLoss: 0.359389\n",
            "Train Epoch: 2 [24016/33808 (71%)]\tLoss: 0.286643\n",
            "Train Epoch: 2 [24336/33808 (72%)]\tLoss: 0.171909\n",
            "Train Epoch: 2 [24656/33808 (73%)]\tLoss: 0.168784\n",
            "Train Epoch: 2 [24976/33808 (74%)]\tLoss: 0.189743\n",
            "Train Epoch: 2 [25296/33808 (75%)]\tLoss: 0.165390\n",
            "Train Epoch: 2 [25616/33808 (76%)]\tLoss: 0.453494\n",
            "Train Epoch: 2 [25936/33808 (77%)]\tLoss: 0.295110\n",
            "Train Epoch: 2 [26256/33808 (78%)]\tLoss: 0.110557\n",
            "Train Epoch: 2 [26576/33808 (79%)]\tLoss: 0.137409\n",
            "Train Epoch: 2 [26896/33808 (80%)]\tLoss: 0.085035\n",
            "Train Epoch: 2 [27216/33808 (80%)]\tLoss: 0.148502\n",
            "Train Epoch: 2 [27536/33808 (81%)]\tLoss: 0.137023\n",
            "Train Epoch: 2 [27856/33808 (82%)]\tLoss: 0.160237\n",
            "Train Epoch: 2 [28176/33808 (83%)]\tLoss: 0.455236\n",
            "Train Epoch: 2 [28496/33808 (84%)]\tLoss: 0.174130\n",
            "Train Epoch: 2 [28816/33808 (85%)]\tLoss: 0.239222\n",
            "Train Epoch: 2 [29136/33808 (86%)]\tLoss: 0.306535\n",
            "Train Epoch: 2 [29456/33808 (87%)]\tLoss: 0.242648\n",
            "Train Epoch: 2 [29776/33808 (88%)]\tLoss: 0.273681\n",
            "Train Epoch: 2 [30096/33808 (89%)]\tLoss: 0.270819\n",
            "Train Epoch: 2 [30416/33808 (90%)]\tLoss: 0.181830\n",
            "Train Epoch: 2 [30736/33808 (91%)]\tLoss: 0.175324\n",
            "Train Epoch: 2 [31056/33808 (92%)]\tLoss: 0.205660\n",
            "Train Epoch: 2 [31376/33808 (93%)]\tLoss: 0.238288\n",
            "Train Epoch: 2 [31696/33808 (94%)]\tLoss: 0.150091\n",
            "Train Epoch: 2 [32016/33808 (95%)]\tLoss: 0.152177\n",
            "Train Epoch: 2 [32336/33808 (96%)]\tLoss: 0.551328\n",
            "Train Epoch: 2 [32656/33808 (97%)]\tLoss: 0.189207\n",
            "Train Epoch: 2 [32976/33808 (97%)]\tLoss: 0.171751\n",
            "Train Epoch: 2 [33296/33808 (98%)]\tLoss: 0.222807\n",
            "Train Epoch: 2 [33616/33808 (99%)]\tLoss: 0.248494\n",
            "\n",
            " Average loss for Epoch 2: 0.2496049274085288 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_AbJRcZf0gL",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03w7H9tdf3ge",
        "colab_type": "code",
        "outputId": "bedd8411-13b6-4cfb-d140-b1b42ab24bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# create dataloader \n",
        "img_path = '/content/drive/My Drive/Steering_Datasets_Rev1 (1)/CH2_002_Test/center/'\n",
        "test_data = TestSet(image_path=img_path, data_transform=transforms.ToTensor()) \n",
        "test_loader = DataLoader(dataset=test_data, batch_size=14, shuffle=True)\n",
        "\n",
        "def test(model, data_loader): \n",
        "  model.eval() \n",
        "\n",
        "  # define loss \n",
        "  criterion = ClassificationLoss() \n",
        "  \n",
        "  test_loss = 0 \n",
        "  with torch.no_grad(): \n",
        "    for batch_idx, (data, target) in enumerate(data_loader): \n",
        "      output = model(data)\n",
        "      loss = criterion(output,target)\n",
        "      test_loss += loss.item() \n",
        "\n",
        "      print(f'Batch_idx: {batch_idx}   Loss: {loss}')\n",
        "\n",
        "    total_loss = test_loss/len(data_loader) \n",
        "    print(f'Total Loss: {total_loss}')\n",
        "    return total_loss \n",
        "\n",
        "\n",
        "\n",
        "test(alexnet, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.int64'>\n",
            "Batch_idx: 0   Loss: 0.23220106959342957\n",
            "Batch_idx: 1   Loss: 0.1281196027994156\n",
            "Batch_idx: 2   Loss: 0.1493135392665863\n",
            "Batch_idx: 3   Loss: 0.31417790055274963\n",
            "Batch_idx: 4   Loss: 0.2530134618282318\n",
            "Batch_idx: 5   Loss: 0.31970450282096863\n",
            "Batch_idx: 6   Loss: 0.19995659589767456\n",
            "Batch_idx: 7   Loss: 0.1720093935728073\n",
            "Batch_idx: 8   Loss: 0.16655562818050385\n",
            "Batch_idx: 9   Loss: 0.1587594747543335\n",
            "Batch_idx: 10   Loss: 0.32486799359321594\n",
            "Batch_idx: 11   Loss: 0.11805446445941925\n",
            "Batch_idx: 12   Loss: 0.12177381664514542\n",
            "Batch_idx: 13   Loss: 0.15568186342716217\n",
            "Batch_idx: 14   Loss: 0.2006593644618988\n",
            "Batch_idx: 15   Loss: 0.2631950080394745\n",
            "Batch_idx: 16   Loss: 0.20328184962272644\n",
            "Batch_idx: 17   Loss: 0.20013852417469025\n",
            "Batch_idx: 18   Loss: 0.13915075361728668\n",
            "Batch_idx: 19   Loss: 0.2807193100452423\n",
            "Batch_idx: 20   Loss: 0.19701215624809265\n",
            "Batch_idx: 21   Loss: 0.30892518162727356\n",
            "Batch_idx: 22   Loss: 0.22163912653923035\n",
            "Batch_idx: 23   Loss: 0.12952283024787903\n",
            "Batch_idx: 24   Loss: 0.2105761468410492\n",
            "Batch_idx: 25   Loss: 0.18106591701507568\n",
            "Batch_idx: 26   Loss: 0.2950966954231262\n",
            "Batch_idx: 27   Loss: 0.22259680926799774\n",
            "Batch_idx: 28   Loss: 0.1568705439567566\n",
            "Batch_idx: 29   Loss: 0.1892714500427246\n",
            "Batch_idx: 30   Loss: 0.10319492965936661\n",
            "Batch_idx: 31   Loss: 0.08197993785142899\n",
            "Batch_idx: 32   Loss: 0.27370837330818176\n",
            "Batch_idx: 33   Loss: 0.2574332356452942\n",
            "Batch_idx: 34   Loss: 0.14047279953956604\n",
            "Batch_idx: 35   Loss: 0.17703527212142944\n",
            "Batch_idx: 36   Loss: 0.16242049634456635\n",
            "Batch_idx: 37   Loss: 0.16545280814170837\n",
            "Batch_idx: 38   Loss: 0.184820756316185\n",
            "Batch_idx: 39   Loss: 0.2573332190513611\n",
            "Batch_idx: 40   Loss: 0.13938018679618835\n",
            "Batch_idx: 41   Loss: 0.3012317419052124\n",
            "Batch_idx: 42   Loss: 0.29460012912750244\n",
            "Batch_idx: 43   Loss: 0.17606110870838165\n",
            "Batch_idx: 44   Loss: 0.19192539155483246\n",
            "Batch_idx: 45   Loss: 0.11262327432632446\n",
            "Batch_idx: 46   Loss: 0.11909786611795425\n",
            "Batch_idx: 47   Loss: 0.11433548480272293\n",
            "Batch_idx: 48   Loss: 0.11551707237958908\n",
            "Batch_idx: 49   Loss: 0.17814084887504578\n",
            "Batch_idx: 50   Loss: 0.14866547286510468\n",
            "Batch_idx: 51   Loss: 0.21863572299480438\n",
            "Batch_idx: 52   Loss: 0.21228410303592682\n",
            "Batch_idx: 53   Loss: 0.16971571743488312\n",
            "Batch_idx: 54   Loss: 0.15788044035434723\n",
            "Batch_idx: 55   Loss: 0.18840619921684265\n",
            "Batch_idx: 56   Loss: 0.34684059023857117\n",
            "Batch_idx: 57   Loss: 0.1423589140176773\n",
            "Batch_idx: 58   Loss: 0.2181430160999298\n",
            "Batch_idx: 59   Loss: 0.1464807242155075\n",
            "Batch_idx: 60   Loss: 0.19369032979011536\n",
            "Batch_idx: 61   Loss: 0.07613929361104965\n",
            "Batch_idx: 62   Loss: 0.09376446157693863\n",
            "Batch_idx: 63   Loss: 0.28115832805633545\n",
            "Batch_idx: 64   Loss: 0.30399295687675476\n",
            "Batch_idx: 65   Loss: 0.20323792099952698\n",
            "Batch_idx: 66   Loss: 0.18199583888053894\n",
            "Batch_idx: 67   Loss: 0.07027170062065125\n",
            "Batch_idx: 68   Loss: 0.14789624512195587\n",
            "Batch_idx: 69   Loss: 0.1328570544719696\n",
            "Batch_idx: 70   Loss: 0.2742278575897217\n",
            "Batch_idx: 71   Loss: 0.13213741779327393\n",
            "Batch_idx: 72   Loss: 0.15750765800476074\n",
            "Batch_idx: 73   Loss: 0.17320816218852997\n",
            "Batch_idx: 74   Loss: 0.1622294932603836\n",
            "Batch_idx: 75   Loss: 0.16723766922950745\n",
            "Batch_idx: 76   Loss: 0.1903485506772995\n",
            "Batch_idx: 77   Loss: 0.14139963686466217\n",
            "Batch_idx: 78   Loss: 0.18247656524181366\n",
            "Batch_idx: 79   Loss: 0.2416984736919403\n",
            "Batch_idx: 80   Loss: 0.13582581281661987\n",
            "Batch_idx: 81   Loss: 0.10480085015296936\n",
            "Batch_idx: 82   Loss: 0.29427972435951233\n",
            "Batch_idx: 83   Loss: 0.10447236150503159\n",
            "Batch_idx: 84   Loss: 0.35176190733909607\n",
            "Batch_idx: 85   Loss: 0.1635592132806778\n",
            "Batch_idx: 86   Loss: 0.14695799350738525\n",
            "Batch_idx: 87   Loss: 0.2506117820739746\n",
            "Batch_idx: 88   Loss: 0.15942035615444183\n",
            "Batch_idx: 89   Loss: 0.14128878712654114\n",
            "Batch_idx: 90   Loss: 0.20262879133224487\n",
            "Batch_idx: 91   Loss: 0.23080560564994812\n",
            "Batch_idx: 92   Loss: 0.1917608380317688\n",
            "Batch_idx: 93   Loss: 0.2696484327316284\n",
            "Batch_idx: 94   Loss: 0.24601416289806366\n",
            "Batch_idx: 95   Loss: 0.28386855125427246\n",
            "Batch_idx: 96   Loss: 0.3042195737361908\n",
            "Batch_idx: 97   Loss: 0.224699929356575\n",
            "Batch_idx: 98   Loss: 0.27748391032218933\n",
            "Batch_idx: 99   Loss: 0.15760396420955658\n",
            "Batch_idx: 100   Loss: 0.1875767856836319\n",
            "Batch_idx: 101   Loss: 0.15922319889068604\n",
            "Batch_idx: 102   Loss: 0.1573726087808609\n",
            "Batch_idx: 103   Loss: 0.2312813401222229\n",
            "Batch_idx: 104   Loss: 0.15431270003318787\n",
            "Batch_idx: 105   Loss: 0.23636622726917267\n",
            "Batch_idx: 106   Loss: 0.14198362827301025\n",
            "Batch_idx: 107   Loss: 0.23441189527511597\n",
            "Batch_idx: 108   Loss: 0.1909019500017166\n",
            "Batch_idx: 109   Loss: 0.1231151595711708\n",
            "Batch_idx: 110   Loss: 0.14090272784233093\n",
            "Batch_idx: 111   Loss: 0.23144574463367462\n",
            "Batch_idx: 112   Loss: 0.19093194603919983\n",
            "Batch_idx: 113   Loss: 0.12564323842525482\n",
            "Batch_idx: 114   Loss: 0.25690126419067383\n",
            "Batch_idx: 115   Loss: 0.16255339980125427\n",
            "Batch_idx: 116   Loss: 0.30914339423179626\n",
            "Batch_idx: 117   Loss: 0.21434740722179413\n",
            "Batch_idx: 118   Loss: 0.2519281506538391\n",
            "Batch_idx: 119   Loss: 0.11861395090818405\n",
            "Batch_idx: 120   Loss: 0.2662770450115204\n",
            "Batch_idx: 121   Loss: 0.20444069802761078\n",
            "Batch_idx: 122   Loss: 0.12938497960567474\n",
            "Batch_idx: 123   Loss: 0.22258786857128143\n",
            "Batch_idx: 124   Loss: 0.25194382667541504\n",
            "Batch_idx: 125   Loss: 0.16766250133514404\n",
            "Batch_idx: 126   Loss: 0.300435870885849\n",
            "Batch_idx: 127   Loss: 0.28679218888282776\n",
            "Batch_idx: 128   Loss: 0.1443713754415512\n",
            "Batch_idx: 129   Loss: 0.22834451496601105\n",
            "Batch_idx: 130   Loss: 0.32006555795669556\n",
            "Batch_idx: 131   Loss: 0.14873602986335754\n",
            "Batch_idx: 132   Loss: 0.13098600506782532\n",
            "Batch_idx: 133   Loss: 0.17112961411476135\n",
            "Batch_idx: 134   Loss: 0.18379239737987518\n",
            "Batch_idx: 135   Loss: 0.22477391362190247\n",
            "Batch_idx: 136   Loss: 0.1589823216199875\n",
            "Batch_idx: 137   Loss: 0.16520094871520996\n",
            "Batch_idx: 138   Loss: 0.2662758529186249\n",
            "Batch_idx: 139   Loss: 0.09205681830644608\n",
            "Batch_idx: 140   Loss: 0.2612769305706024\n",
            "Batch_idx: 141   Loss: 0.22279182076454163\n",
            "Batch_idx: 142   Loss: 0.1426055133342743\n",
            "Batch_idx: 143   Loss: 0.2251211702823639\n",
            "Batch_idx: 144   Loss: 0.1345127671957016\n",
            "Batch_idx: 145   Loss: 0.2949325144290924\n",
            "Batch_idx: 146   Loss: 0.28669729828834534\n",
            "Batch_idx: 147   Loss: 0.28179025650024414\n",
            "Batch_idx: 148   Loss: 0.3067081570625305\n",
            "Batch_idx: 149   Loss: 0.3342335820198059\n",
            "Batch_idx: 150   Loss: 0.1605200469493866\n",
            "Batch_idx: 151   Loss: 0.1701628565788269\n",
            "Batch_idx: 152   Loss: 0.16543008387088776\n",
            "Batch_idx: 153   Loss: 0.19694291055202484\n",
            "Batch_idx: 154   Loss: 0.11742143332958221\n",
            "Batch_idx: 155   Loss: 0.24137535691261292\n",
            "Batch_idx: 156   Loss: 0.23064424097537994\n",
            "Batch_idx: 157   Loss: 0.3346750736236572\n",
            "Batch_idx: 158   Loss: 0.2044830620288849\n",
            "Batch_idx: 159   Loss: 0.15219663083553314\n",
            "Batch_idx: 160   Loss: 0.29156172275543213\n",
            "Batch_idx: 161   Loss: 0.13445048034191132\n",
            "Batch_idx: 162   Loss: 0.17835040390491486\n",
            "Batch_idx: 163   Loss: 0.20466046035289764\n",
            "Batch_idx: 164   Loss: 0.2586634159088135\n",
            "Batch_idx: 165   Loss: 0.2158023566007614\n",
            "Batch_idx: 166   Loss: 0.18288825452327728\n",
            "Batch_idx: 167   Loss: 0.19181321561336517\n",
            "Batch_idx: 168   Loss: 0.12192536145448685\n",
            "Batch_idx: 169   Loss: 0.17780883610248566\n",
            "Batch_idx: 170   Loss: 0.26128751039505005\n",
            "Batch_idx: 171   Loss: 0.1546132117509842\n",
            "Batch_idx: 172   Loss: 0.1190759539604187\n",
            "Batch_idx: 173   Loss: 0.1894676238298416\n",
            "Batch_idx: 174   Loss: 0.20255273580551147\n",
            "Batch_idx: 175   Loss: 0.1702294498682022\n",
            "Batch_idx: 176   Loss: 0.24664506316184998\n",
            "Batch_idx: 177   Loss: 0.21861281991004944\n",
            "Batch_idx: 178   Loss: 0.16870099306106567\n",
            "Batch_idx: 179   Loss: 0.20861342549324036\n",
            "Batch_idx: 180   Loss: 0.1850743293762207\n",
            "Batch_idx: 181   Loss: 0.17080724239349365\n",
            "Batch_idx: 182   Loss: 0.1753702610731125\n",
            "Batch_idx: 183   Loss: 0.13970953226089478\n",
            "Batch_idx: 184   Loss: 0.22559835016727448\n",
            "Batch_idx: 185   Loss: 0.13818196952342987\n",
            "Batch_idx: 186   Loss: 0.16507279872894287\n",
            "Batch_idx: 187   Loss: 0.127464160323143\n",
            "Batch_idx: 188   Loss: 0.23798389732837677\n",
            "Batch_idx: 189   Loss: 0.2635209560394287\n",
            "Batch_idx: 190   Loss: 0.22067441046237946\n",
            "Batch_idx: 191   Loss: 0.1560429483652115\n",
            "Batch_idx: 192   Loss: 0.1937822550535202\n",
            "Batch_idx: 193   Loss: 0.12944936752319336\n",
            "Batch_idx: 194   Loss: 0.16536547243595123\n",
            "Batch_idx: 195   Loss: 0.16802655160427094\n",
            "Batch_idx: 196   Loss: 0.15642936527729034\n",
            "Batch_idx: 197   Loss: 0.26693040132522583\n",
            "Batch_idx: 198   Loss: 0.21045322716236115\n",
            "Batch_idx: 199   Loss: 0.14788398146629333\n",
            "Batch_idx: 200   Loss: 0.23463982343673706\n",
            "Batch_idx: 201   Loss: 0.1500110924243927\n",
            "Batch_idx: 202   Loss: 0.17816860973834991\n",
            "Batch_idx: 203   Loss: 0.2474106252193451\n",
            "Batch_idx: 204   Loss: 0.08958180993795395\n",
            "Batch_idx: 205   Loss: 0.19506347179412842\n",
            "Batch_idx: 206   Loss: 0.23788970708847046\n",
            "Batch_idx: 207   Loss: 0.27873876690864563\n",
            "Batch_idx: 208   Loss: 0.2622334361076355\n",
            "Batch_idx: 209   Loss: 0.08655059337615967\n",
            "Batch_idx: 210   Loss: 0.141657292842865\n",
            "Batch_idx: 211   Loss: 0.23279409110546112\n",
            "Batch_idx: 212   Loss: 0.1531466543674469\n",
            "Batch_idx: 213   Loss: 0.20622238516807556\n",
            "Batch_idx: 214   Loss: 0.16872356832027435\n",
            "Batch_idx: 215   Loss: 0.223631352186203\n",
            "Batch_idx: 216   Loss: 0.20388363301753998\n",
            "Batch_idx: 217   Loss: 0.09362461417913437\n",
            "Batch_idx: 218   Loss: 0.13223041594028473\n",
            "Batch_idx: 219   Loss: 0.15682452917099\n",
            "Batch_idx: 220   Loss: 0.22831100225448608\n",
            "Batch_idx: 221   Loss: 0.14501439034938812\n",
            "Batch_idx: 222   Loss: 0.3816032111644745\n",
            "Batch_idx: 223   Loss: 0.20534230768680573\n",
            "Batch_idx: 224   Loss: 0.12741650640964508\n",
            "Batch_idx: 225   Loss: 0.13642673194408417\n",
            "Batch_idx: 226   Loss: 0.296682208776474\n",
            "Batch_idx: 227   Loss: 0.23782996833324432\n",
            "Batch_idx: 228   Loss: 0.19809959828853607\n",
            "Batch_idx: 229   Loss: 0.19201356172561646\n",
            "Batch_idx: 230   Loss: 0.3050248920917511\n",
            "Batch_idx: 231   Loss: 0.1759718954563141\n",
            "Batch_idx: 232   Loss: 0.22046500444412231\n",
            "Batch_idx: 233   Loss: 0.09934291988611221\n",
            "Batch_idx: 234   Loss: 0.25881972908973694\n",
            "Batch_idx: 235   Loss: 0.19672027230262756\n",
            "Batch_idx: 236   Loss: 0.16924260556697845\n",
            "Batch_idx: 237   Loss: 0.12012597918510437\n",
            "Batch_idx: 238   Loss: 0.1524801254272461\n",
            "Batch_idx: 239   Loss: 0.22471150755882263\n",
            "Batch_idx: 240   Loss: 0.16039958596229553\n",
            "Batch_idx: 241   Loss: 0.32203665375709534\n",
            "Batch_idx: 242   Loss: 0.1838987022638321\n",
            "Batch_idx: 243   Loss: 0.23136307299137115\n",
            "Batch_idx: 244   Loss: 0.2096284180879593\n",
            "Batch_idx: 245   Loss: 0.1381390243768692\n",
            "Batch_idx: 246   Loss: 0.23943182826042175\n",
            "Batch_idx: 247   Loss: 0.15428464114665985\n",
            "Batch_idx: 248   Loss: 0.2995433807373047\n",
            "Batch_idx: 249   Loss: 0.13400253653526306\n",
            "Batch_idx: 250   Loss: 0.3423653841018677\n",
            "Batch_idx: 251   Loss: 0.2404240071773529\n",
            "Batch_idx: 252   Loss: 0.2199704349040985\n",
            "Batch_idx: 253   Loss: 0.21491342782974243\n",
            "Batch_idx: 254   Loss: 0.17277874052524567\n",
            "Batch_idx: 255   Loss: 0.12655626237392426\n",
            "Batch_idx: 256   Loss: 0.3205835223197937\n",
            "Batch_idx: 257   Loss: 0.09615856409072876\n",
            "Batch_idx: 258   Loss: 0.3459043502807617\n",
            "Batch_idx: 259   Loss: 0.24945983290672302\n",
            "Batch_idx: 260   Loss: 0.08742066472768784\n",
            "Batch_idx: 261   Loss: 0.244296595454216\n",
            "Batch_idx: 262   Loss: 0.28795257210731506\n",
            "Batch_idx: 263   Loss: 0.30381494760513306\n",
            "Batch_idx: 264   Loss: 0.09813417494297028\n",
            "Batch_idx: 265   Loss: 0.3058464825153351\n",
            "Batch_idx: 266   Loss: 0.21476584672927856\n",
            "Batch_idx: 267   Loss: 0.2157536894083023\n",
            "Batch_idx: 268   Loss: 0.11852006614208221\n",
            "Batch_idx: 269   Loss: 0.12640883028507233\n",
            "Batch_idx: 270   Loss: 0.1323660910129547\n",
            "Batch_idx: 271   Loss: 0.1812373697757721\n",
            "Batch_idx: 272   Loss: 0.16600275039672852\n",
            "Batch_idx: 273   Loss: 0.21389980614185333\n",
            "Batch_idx: 274   Loss: 0.2032339870929718\n",
            "Batch_idx: 275   Loss: 0.10670091211795807\n",
            "Batch_idx: 276   Loss: 0.17253628373146057\n",
            "Batch_idx: 277   Loss: 0.1809164136648178\n",
            "Batch_idx: 278   Loss: 0.16219136118888855\n",
            "Batch_idx: 279   Loss: 0.113443523645401\n",
            "Batch_idx: 280   Loss: 0.2526973485946655\n",
            "Batch_idx: 281   Loss: 0.10291355848312378\n",
            "Batch_idx: 282   Loss: 0.26529228687286377\n",
            "Batch_idx: 283   Loss: 0.2363048493862152\n",
            "Batch_idx: 284   Loss: 0.1843259334564209\n",
            "Batch_idx: 285   Loss: 0.19300197064876556\n",
            "Batch_idx: 286   Loss: 0.2133493423461914\n",
            "Batch_idx: 287   Loss: 0.2511856257915497\n",
            "Batch_idx: 288   Loss: 0.1670364886522293\n",
            "Batch_idx: 289   Loss: 0.27800220251083374\n",
            "Batch_idx: 290   Loss: 0.27536433935165405\n",
            "Batch_idx: 291   Loss: 0.29704564809799194\n",
            "Batch_idx: 292   Loss: 0.22300615906715393\n",
            "Batch_idx: 293   Loss: 0.23375467956066132\n",
            "Batch_idx: 294   Loss: 0.13361065089702606\n",
            "Batch_idx: 295   Loss: 0.1762784719467163\n",
            "Batch_idx: 296   Loss: 0.17439597845077515\n",
            "Batch_idx: 297   Loss: 0.07841183990240097\n",
            "Batch_idx: 298   Loss: 0.10475810617208481\n",
            "Batch_idx: 299   Loss: 0.19075068831443787\n",
            "Batch_idx: 300   Loss: 0.1620093733072281\n",
            "Batch_idx: 301   Loss: 0.1241103783249855\n",
            "Batch_idx: 302   Loss: 0.1356228142976761\n",
            "Batch_idx: 303   Loss: 0.24013572931289673\n",
            "Batch_idx: 304   Loss: 0.26501721143722534\n",
            "Batch_idx: 305   Loss: 0.142475888133049\n",
            "Batch_idx: 306   Loss: 0.20418456196784973\n",
            "Batch_idx: 307   Loss: 0.3112722933292389\n",
            "Batch_idx: 308   Loss: 0.18935763835906982\n",
            "Batch_idx: 309   Loss: 0.15130776166915894\n",
            "Batch_idx: 310   Loss: 0.1840931624174118\n",
            "Batch_idx: 311   Loss: 0.15163372457027435\n",
            "Batch_idx: 312   Loss: 0.18235424160957336\n",
            "Batch_idx: 313   Loss: 0.18380038440227509\n",
            "Batch_idx: 314   Loss: 0.17882439494132996\n",
            "Batch_idx: 315   Loss: 0.1809445321559906\n",
            "Batch_idx: 316   Loss: 0.10138050466775894\n",
            "Batch_idx: 317   Loss: 0.18422608077526093\n",
            "Batch_idx: 318   Loss: 0.17201590538024902\n",
            "Batch_idx: 319   Loss: 0.0627380982041359\n",
            "Batch_idx: 320   Loss: 0.3047410845756531\n",
            "Batch_idx: 321   Loss: 0.18510693311691284\n",
            "Batch_idx: 322   Loss: 0.17968755960464478\n",
            "Batch_idx: 323   Loss: 0.2593236565589905\n",
            "Batch_idx: 324   Loss: 0.15689240396022797\n",
            "Batch_idx: 325   Loss: 0.2131408452987671\n",
            "Batch_idx: 326   Loss: 0.1663219928741455\n",
            "Batch_idx: 327   Loss: 0.10211759060621262\n",
            "Batch_idx: 328   Loss: 0.2616634666919708\n",
            "Batch_idx: 329   Loss: 0.37697485089302063\n",
            "Batch_idx: 330   Loss: 0.3368587791919708\n",
            "Batch_idx: 331   Loss: 0.12744666635990143\n",
            "Batch_idx: 332   Loss: 0.22494788467884064\n",
            "Batch_idx: 333   Loss: 0.10776206105947495\n",
            "Batch_idx: 334   Loss: 0.3209318220615387\n",
            "Batch_idx: 335   Loss: 0.23951134085655212\n",
            "Batch_idx: 336   Loss: 0.2925666570663452\n",
            "Batch_idx: 337   Loss: 0.2503788471221924\n",
            "Batch_idx: 338   Loss: 0.16584788262844086\n",
            "Batch_idx: 339   Loss: 0.194552481174469\n",
            "Batch_idx: 340   Loss: 0.18090778589248657\n",
            "Batch_idx: 341   Loss: 0.21784014999866486\n",
            "Batch_idx: 342   Loss: 0.12859360873699188\n",
            "Batch_idx: 343   Loss: 0.43806740641593933\n",
            "Batch_idx: 344   Loss: 0.18755021691322327\n",
            "Batch_idx: 345   Loss: 0.31213507056236267\n",
            "Batch_idx: 346   Loss: 0.25272136926651\n",
            "Batch_idx: 347   Loss: 0.10351160168647766\n",
            "Batch_idx: 348   Loss: 0.12879568338394165\n",
            "Batch_idx: 349   Loss: 0.14028875529766083\n",
            "Batch_idx: 350   Loss: 0.21115969121456146\n",
            "Batch_idx: 351   Loss: 0.09232567250728607\n",
            "Batch_idx: 352   Loss: 0.22052377462387085\n",
            "Batch_idx: 353   Loss: 0.20222344994544983\n",
            "Batch_idx: 354   Loss: 0.2831040918827057\n",
            "Batch_idx: 355   Loss: 0.2632918655872345\n",
            "Batch_idx: 356   Loss: 0.21281996369361877\n",
            "Batch_idx: 357   Loss: 0.11667042970657349\n",
            "Batch_idx: 358   Loss: 0.19812294840812683\n",
            "Batch_idx: 359   Loss: 0.1874563992023468\n",
            "Batch_idx: 360   Loss: 0.24499261379241943\n",
            "Batch_idx: 361   Loss: 0.1868506520986557\n",
            "Batch_idx: 362   Loss: 0.15981265902519226\n",
            "Batch_idx: 363   Loss: 0.2626188099384308\n",
            "Batch_idx: 364   Loss: 0.18912754952907562\n",
            "Batch_idx: 365   Loss: 0.11262411624193192\n",
            "Batch_idx: 366   Loss: 0.23131288588047028\n",
            "Batch_idx: 367   Loss: 0.31212669610977173\n",
            "Batch_idx: 368   Loss: 0.16677619516849518\n",
            "Batch_idx: 369   Loss: 0.18376266956329346\n",
            "Batch_idx: 370   Loss: 0.2682695984840393\n",
            "Batch_idx: 371   Loss: 0.2106817364692688\n",
            "Batch_idx: 372   Loss: 0.18474051356315613\n",
            "Batch_idx: 373   Loss: 0.13960322737693787\n",
            "Batch_idx: 374   Loss: 0.1574779450893402\n",
            "Batch_idx: 375   Loss: 0.22399933636188507\n",
            "Batch_idx: 376   Loss: 0.11939677596092224\n",
            "Batch_idx: 377   Loss: 0.1690797358751297\n",
            "Batch_idx: 378   Loss: 0.12915922701358795\n",
            "Batch_idx: 379   Loss: 0.20369017124176025\n",
            "Batch_idx: 380   Loss: 0.17314505577087402\n",
            "Batch_idx: 381   Loss: 0.16525693237781525\n",
            "Batch_idx: 382   Loss: 0.17158885300159454\n",
            "Batch_idx: 383   Loss: 0.23176303505897522\n",
            "Batch_idx: 384   Loss: 0.07698333263397217\n",
            "Batch_idx: 385   Loss: 0.24917401373386383\n",
            "Batch_idx: 386   Loss: 0.14200590550899506\n",
            "Batch_idx: 387   Loss: 0.3385871946811676\n",
            "Batch_idx: 388   Loss: 0.3047197163105011\n",
            "Batch_idx: 389   Loss: 0.12193532288074493\n",
            "Batch_idx: 390   Loss: 0.2833482027053833\n",
            "Batch_idx: 391   Loss: 0.15789783000946045\n",
            "Batch_idx: 392   Loss: 0.28771695494651794\n",
            "Batch_idx: 393   Loss: 0.15053360164165497\n",
            "Batch_idx: 394   Loss: 0.18231886625289917\n",
            "Batch_idx: 395   Loss: 0.16164207458496094\n",
            "Batch_idx: 396   Loss: 0.2595123052597046\n",
            "Batch_idx: 397   Loss: 0.23122519254684448\n",
            "Batch_idx: 398   Loss: 0.18740510940551758\n",
            "Batch_idx: 399   Loss: 0.18946538865566254\n",
            "Batch_idx: 400   Loss: 0.29940587282180786\n",
            "Total Loss: 0.1980950214210294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1980950214210294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5PZq7oemMG1",
        "colab_type": "text"
      },
      "source": [
        "## My network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDiQ5i0MmLxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn \n",
        "# My Model \n",
        "\n",
        "class CNNModel(torch.nn.Module): \n",
        "  def __init__(self): \n",
        "    super(CNNModel, self).__init__() \n",
        "\n",
        "    self.conv_net = nn.Sequential(\n",
        "        # Conv block 1 \n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2), \n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "        # Conv block 2 \n",
        "        nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding=2), \n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "        # Conv block 3 \n",
        "        nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1), \n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # Conv block 4 \n",
        "        nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # Conv block 5 \n",
        "        nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential( \n",
        "        nn.Dropout(p=0.5), \n",
        "        nn.Linear(in_features=13824, out_features=4096, bias=True), \n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.5), \n",
        "        nn.Linear(in_features=4096, out_features=4096, bias=True), \n",
        "        nn.ReLU(inplace=True), \n",
        "        nn.Linear(in_features=4096, out_features=1, bias=True)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x): \n",
        "    x = self.conv_net(x)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.fc(x)\n",
        "    return x \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb_FOfESyg2f",
        "colab_type": "code",
        "outputId": "efd1a386-c0e9-4926-f692-7cf9f437baa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "myModel = CNNModel()\n",
        "train(args,myModel,train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [16/33808 (0%)]\tLoss: 0.256568\n",
            "Train Epoch: 1 [336/33808 (1%)]\tLoss: 0.128553\n",
            "Train Epoch: 1 [656/33808 (2%)]\tLoss: 0.275326\n",
            "Train Epoch: 1 [976/33808 (3%)]\tLoss: 0.076100\n",
            "Train Epoch: 1 [1296/33808 (4%)]\tLoss: 0.199798\n",
            "Train Epoch: 1 [1616/33808 (5%)]\tLoss: 0.358178\n",
            "Train Epoch: 1 [1936/33808 (6%)]\tLoss: 0.511225\n",
            "Train Epoch: 1 [2256/33808 (7%)]\tLoss: 0.204554\n",
            "Train Epoch: 1 [2576/33808 (8%)]\tLoss: 0.314032\n",
            "Train Epoch: 1 [2896/33808 (9%)]\tLoss: 0.379963\n",
            "Train Epoch: 1 [3216/33808 (9%)]\tLoss: 0.290904\n",
            "Train Epoch: 1 [3536/33808 (10%)]\tLoss: 0.161201\n",
            "Train Epoch: 1 [3856/33808 (11%)]\tLoss: 0.155005\n",
            "Train Epoch: 1 [4176/33808 (12%)]\tLoss: 0.199217\n",
            "Train Epoch: 1 [4496/33808 (13%)]\tLoss: 0.206063\n",
            "Train Epoch: 1 [4816/33808 (14%)]\tLoss: 0.269928\n",
            "Train Epoch: 1 [5136/33808 (15%)]\tLoss: 0.220694\n",
            "Train Epoch: 1 [5456/33808 (16%)]\tLoss: 0.248503\n",
            "Train Epoch: 1 [5776/33808 (17%)]\tLoss: 0.188702\n",
            "Train Epoch: 1 [6096/33808 (18%)]\tLoss: 0.202316\n",
            "Train Epoch: 1 [6416/33808 (19%)]\tLoss: 0.155693\n",
            "Train Epoch: 1 [6736/33808 (20%)]\tLoss: 0.361492\n",
            "Train Epoch: 1 [7056/33808 (21%)]\tLoss: 0.129305\n",
            "Train Epoch: 1 [7376/33808 (22%)]\tLoss: 0.188646\n",
            "Train Epoch: 1 [7696/33808 (23%)]\tLoss: 0.600133\n",
            "Train Epoch: 1 [8016/33808 (24%)]\tLoss: 0.229774\n",
            "Train Epoch: 1 [8336/33808 (25%)]\tLoss: 0.126814\n",
            "Train Epoch: 1 [8656/33808 (26%)]\tLoss: 0.290777\n",
            "Train Epoch: 1 [8976/33808 (27%)]\tLoss: 0.116542\n",
            "Train Epoch: 1 [9296/33808 (27%)]\tLoss: 0.245872\n",
            "Train Epoch: 1 [9616/33808 (28%)]\tLoss: 0.290992\n",
            "Train Epoch: 1 [9936/33808 (29%)]\tLoss: 0.339471\n",
            "Train Epoch: 1 [10256/33808 (30%)]\tLoss: 0.159771\n",
            "Train Epoch: 1 [10576/33808 (31%)]\tLoss: 0.325864\n",
            "Train Epoch: 1 [10896/33808 (32%)]\tLoss: 0.318490\n",
            "Train Epoch: 1 [11216/33808 (33%)]\tLoss: 0.212714\n",
            "Train Epoch: 1 [11536/33808 (34%)]\tLoss: 0.118474\n",
            "Train Epoch: 1 [11856/33808 (35%)]\tLoss: 0.311614\n",
            "Train Epoch: 1 [12176/33808 (36%)]\tLoss: 0.194639\n",
            "Train Epoch: 1 [12496/33808 (37%)]\tLoss: 0.095934\n",
            "Train Epoch: 1 [12816/33808 (38%)]\tLoss: 0.156216\n",
            "Train Epoch: 1 [13136/33808 (39%)]\tLoss: 0.250841\n",
            "Train Epoch: 1 [13456/33808 (40%)]\tLoss: 0.164671\n",
            "Train Epoch: 1 [13776/33808 (41%)]\tLoss: 0.322706\n",
            "Train Epoch: 1 [14096/33808 (42%)]\tLoss: 0.277756\n",
            "Train Epoch: 1 [14416/33808 (43%)]\tLoss: 0.384206\n",
            "Train Epoch: 1 [14736/33808 (44%)]\tLoss: 0.102256\n",
            "Train Epoch: 1 [15056/33808 (44%)]\tLoss: 0.191775\n",
            "Train Epoch: 1 [15376/33808 (45%)]\tLoss: 0.177363\n",
            "Train Epoch: 1 [15696/33808 (46%)]\tLoss: 0.318394\n",
            "Train Epoch: 1 [16016/33808 (47%)]\tLoss: 0.113172\n",
            "Train Epoch: 1 [16336/33808 (48%)]\tLoss: 0.248541\n",
            "Train Epoch: 1 [16656/33808 (49%)]\tLoss: 0.126612\n",
            "Train Epoch: 1 [16976/33808 (50%)]\tLoss: 0.181886\n",
            "Train Epoch: 1 [17296/33808 (51%)]\tLoss: 0.261994\n",
            "Train Epoch: 1 [17616/33808 (52%)]\tLoss: 0.148615\n",
            "Train Epoch: 1 [17936/33808 (53%)]\tLoss: 0.171339\n",
            "Train Epoch: 1 [18256/33808 (54%)]\tLoss: 0.100623\n",
            "Train Epoch: 1 [18576/33808 (55%)]\tLoss: 0.217500\n",
            "Train Epoch: 1 [18896/33808 (56%)]\tLoss: 0.134958\n",
            "Train Epoch: 1 [19216/33808 (57%)]\tLoss: 0.325919\n",
            "Train Epoch: 1 [19536/33808 (58%)]\tLoss: 0.514131\n",
            "Train Epoch: 1 [19856/33808 (59%)]\tLoss: 0.068104\n",
            "Train Epoch: 1 [20176/33808 (60%)]\tLoss: 0.402906\n",
            "Train Epoch: 1 [20496/33808 (61%)]\tLoss: 0.159193\n",
            "Train Epoch: 1 [20816/33808 (62%)]\tLoss: 0.195094\n",
            "Train Epoch: 1 [21136/33808 (62%)]\tLoss: 0.150155\n",
            "Train Epoch: 1 [21456/33808 (63%)]\tLoss: 0.208410\n",
            "Train Epoch: 1 [21776/33808 (64%)]\tLoss: 0.202329\n",
            "Train Epoch: 1 [22096/33808 (65%)]\tLoss: 0.180311\n",
            "Train Epoch: 1 [22416/33808 (66%)]\tLoss: 0.281803\n",
            "Train Epoch: 1 [22736/33808 (67%)]\tLoss: 0.136791\n",
            "Train Epoch: 1 [23056/33808 (68%)]\tLoss: 0.166803\n",
            "Train Epoch: 1 [23376/33808 (69%)]\tLoss: 0.178699\n",
            "Train Epoch: 1 [23696/33808 (70%)]\tLoss: 0.360412\n",
            "Train Epoch: 1 [24016/33808 (71%)]\tLoss: 0.185813\n",
            "Train Epoch: 1 [24336/33808 (72%)]\tLoss: 0.130234\n",
            "Train Epoch: 1 [24656/33808 (73%)]\tLoss: 0.258964\n",
            "Train Epoch: 1 [24976/33808 (74%)]\tLoss: 0.295927\n",
            "Train Epoch: 1 [25296/33808 (75%)]\tLoss: 0.152997\n",
            "Train Epoch: 1 [25616/33808 (76%)]\tLoss: 0.334679\n",
            "Train Epoch: 1 [25936/33808 (77%)]\tLoss: 0.216672\n",
            "Train Epoch: 1 [26256/33808 (78%)]\tLoss: 0.247984\n",
            "Train Epoch: 1 [26576/33808 (79%)]\tLoss: 0.147566\n",
            "Train Epoch: 1 [26896/33808 (80%)]\tLoss: 0.212660\n",
            "Train Epoch: 1 [27216/33808 (80%)]\tLoss: 0.291668\n",
            "Train Epoch: 1 [27536/33808 (81%)]\tLoss: 0.259951\n",
            "Train Epoch: 1 [27856/33808 (82%)]\tLoss: 0.427786\n",
            "Train Epoch: 1 [28176/33808 (83%)]\tLoss: 0.132232\n",
            "Train Epoch: 1 [28496/33808 (84%)]\tLoss: 0.099638\n",
            "Train Epoch: 1 [28816/33808 (85%)]\tLoss: 0.204246\n",
            "Train Epoch: 1 [29136/33808 (86%)]\tLoss: 0.182102\n",
            "Train Epoch: 1 [29456/33808 (87%)]\tLoss: 0.319821\n",
            "Train Epoch: 1 [29776/33808 (88%)]\tLoss: 0.160761\n",
            "Train Epoch: 1 [30096/33808 (89%)]\tLoss: 0.381466\n",
            "Train Epoch: 1 [30416/33808 (90%)]\tLoss: 0.508561\n",
            "Train Epoch: 1 [30736/33808 (91%)]\tLoss: 0.215984\n",
            "Train Epoch: 1 [31056/33808 (92%)]\tLoss: 0.165874\n",
            "Train Epoch: 1 [31376/33808 (93%)]\tLoss: 0.156570\n",
            "Train Epoch: 1 [31696/33808 (94%)]\tLoss: 0.099289\n",
            "Train Epoch: 1 [32016/33808 (95%)]\tLoss: 0.415043\n",
            "Train Epoch: 1 [32336/33808 (96%)]\tLoss: 0.168694\n",
            "Train Epoch: 1 [32656/33808 (97%)]\tLoss: 0.047654\n",
            "Train Epoch: 1 [32976/33808 (97%)]\tLoss: 0.182831\n",
            "Train Epoch: 1 [33296/33808 (98%)]\tLoss: 0.190336\n",
            "Train Epoch: 1 [33616/33808 (99%)]\tLoss: 0.370254\n",
            "\n",
            " Average loss for Epoch 1: 0.25069517711546824 \n",
            "\n",
            "Train Epoch: 2 [16/33808 (0%)]\tLoss: 0.250800\n",
            "Train Epoch: 2 [336/33808 (1%)]\tLoss: 0.276028\n",
            "Train Epoch: 2 [656/33808 (2%)]\tLoss: 0.283162\n",
            "Train Epoch: 2 [976/33808 (3%)]\tLoss: 0.063812\n",
            "Train Epoch: 2 [1296/33808 (4%)]\tLoss: 0.256300\n",
            "Train Epoch: 2 [1616/33808 (5%)]\tLoss: 0.280890\n",
            "Train Epoch: 2 [1936/33808 (6%)]\tLoss: 0.348367\n",
            "Train Epoch: 2 [2256/33808 (7%)]\tLoss: 0.119401\n",
            "Train Epoch: 2 [2576/33808 (8%)]\tLoss: 0.333991\n",
            "Train Epoch: 2 [2896/33808 (9%)]\tLoss: 0.449453\n",
            "Train Epoch: 2 [3216/33808 (9%)]\tLoss: 0.105016\n",
            "Train Epoch: 2 [3536/33808 (10%)]\tLoss: 0.125118\n",
            "Train Epoch: 2 [3856/33808 (11%)]\tLoss: 0.177601\n",
            "Train Epoch: 2 [4176/33808 (12%)]\tLoss: 0.253333\n",
            "Train Epoch: 2 [4496/33808 (13%)]\tLoss: 0.128998\n",
            "Train Epoch: 2 [4816/33808 (14%)]\tLoss: 0.170101\n",
            "Train Epoch: 2 [5136/33808 (15%)]\tLoss: 0.418711\n",
            "Train Epoch: 2 [5456/33808 (16%)]\tLoss: 0.218949\n",
            "Train Epoch: 2 [5776/33808 (17%)]\tLoss: 0.211509\n",
            "Train Epoch: 2 [6096/33808 (18%)]\tLoss: 0.184404\n",
            "Train Epoch: 2 [6416/33808 (19%)]\tLoss: 0.268798\n",
            "Train Epoch: 2 [6736/33808 (20%)]\tLoss: 0.216283\n",
            "Train Epoch: 2 [7056/33808 (21%)]\tLoss: 0.185353\n",
            "Train Epoch: 2 [7376/33808 (22%)]\tLoss: 0.204789\n",
            "Train Epoch: 2 [7696/33808 (23%)]\tLoss: 0.468875\n",
            "Train Epoch: 2 [8016/33808 (24%)]\tLoss: 0.353154\n",
            "Train Epoch: 2 [8336/33808 (25%)]\tLoss: 0.178700\n",
            "Train Epoch: 2 [8656/33808 (26%)]\tLoss: 0.201177\n",
            "Train Epoch: 2 [8976/33808 (27%)]\tLoss: 0.241984\n",
            "Train Epoch: 2 [9296/33808 (27%)]\tLoss: 0.220102\n",
            "Train Epoch: 2 [9616/33808 (28%)]\tLoss: 0.198089\n",
            "Train Epoch: 2 [9936/33808 (29%)]\tLoss: 0.156154\n",
            "Train Epoch: 2 [10256/33808 (30%)]\tLoss: 0.268953\n",
            "Train Epoch: 2 [10576/33808 (31%)]\tLoss: 0.346305\n",
            "Train Epoch: 2 [10896/33808 (32%)]\tLoss: 0.229566\n",
            "Train Epoch: 2 [11216/33808 (33%)]\tLoss: 0.178716\n",
            "Train Epoch: 2 [11536/33808 (34%)]\tLoss: 0.164630\n",
            "Train Epoch: 2 [11856/33808 (35%)]\tLoss: 0.364497\n",
            "Train Epoch: 2 [12176/33808 (36%)]\tLoss: 0.126231\n",
            "Train Epoch: 2 [12496/33808 (37%)]\tLoss: 0.161278\n",
            "Train Epoch: 2 [12816/33808 (38%)]\tLoss: 0.196179\n",
            "Train Epoch: 2 [13136/33808 (39%)]\tLoss: 0.177288\n",
            "Train Epoch: 2 [13456/33808 (40%)]\tLoss: 0.442735\n",
            "Train Epoch: 2 [13776/33808 (41%)]\tLoss: 0.191709\n",
            "Train Epoch: 2 [14096/33808 (42%)]\tLoss: 0.106618\n",
            "Train Epoch: 2 [14416/33808 (43%)]\tLoss: 0.288754\n",
            "Train Epoch: 2 [14736/33808 (44%)]\tLoss: 0.314278\n",
            "Train Epoch: 2 [15056/33808 (44%)]\tLoss: 0.220161\n",
            "Train Epoch: 2 [15376/33808 (45%)]\tLoss: 0.153238\n",
            "Train Epoch: 2 [15696/33808 (46%)]\tLoss: 0.158583\n",
            "Train Epoch: 2 [16016/33808 (47%)]\tLoss: 0.222798\n",
            "Train Epoch: 2 [16336/33808 (48%)]\tLoss: 0.296361\n",
            "Train Epoch: 2 [16656/33808 (49%)]\tLoss: 0.322668\n",
            "Train Epoch: 2 [16976/33808 (50%)]\tLoss: 0.320417\n",
            "Train Epoch: 2 [17296/33808 (51%)]\tLoss: 0.175351\n",
            "Train Epoch: 2 [17616/33808 (52%)]\tLoss: 0.331706\n",
            "Train Epoch: 2 [17936/33808 (53%)]\tLoss: 0.206730\n",
            "Train Epoch: 2 [18256/33808 (54%)]\tLoss: 0.272923\n",
            "Train Epoch: 2 [18576/33808 (55%)]\tLoss: 0.269882\n",
            "Train Epoch: 2 [18896/33808 (56%)]\tLoss: 0.099992\n",
            "Train Epoch: 2 [19216/33808 (57%)]\tLoss: 0.211830\n",
            "Train Epoch: 2 [19536/33808 (58%)]\tLoss: 0.217201\n",
            "Train Epoch: 2 [19856/33808 (59%)]\tLoss: 0.256345\n",
            "Train Epoch: 2 [20176/33808 (60%)]\tLoss: 0.177396\n",
            "Train Epoch: 2 [20496/33808 (61%)]\tLoss: 0.342936\n",
            "Train Epoch: 2 [20816/33808 (62%)]\tLoss: 0.118724\n",
            "Train Epoch: 2 [21136/33808 (62%)]\tLoss: 0.178315\n",
            "Train Epoch: 2 [21456/33808 (63%)]\tLoss: 0.366112\n",
            "Train Epoch: 2 [21776/33808 (64%)]\tLoss: 0.201494\n",
            "Train Epoch: 2 [22096/33808 (65%)]\tLoss: 0.278623\n",
            "Train Epoch: 2 [22416/33808 (66%)]\tLoss: 0.411622\n",
            "Train Epoch: 2 [22736/33808 (67%)]\tLoss: 0.359298\n",
            "Train Epoch: 2 [23056/33808 (68%)]\tLoss: 0.217412\n",
            "Train Epoch: 2 [23376/33808 (69%)]\tLoss: 0.399164\n",
            "Train Epoch: 2 [23696/33808 (70%)]\tLoss: 0.186695\n",
            "Train Epoch: 2 [24016/33808 (71%)]\tLoss: 0.202268\n",
            "Train Epoch: 2 [24336/33808 (72%)]\tLoss: 0.172098\n",
            "Train Epoch: 2 [24656/33808 (73%)]\tLoss: 0.185685\n",
            "Train Epoch: 2 [24976/33808 (74%)]\tLoss: 0.300780\n",
            "Train Epoch: 2 [25296/33808 (75%)]\tLoss: 0.191783\n",
            "Train Epoch: 2 [25616/33808 (76%)]\tLoss: 0.238055\n",
            "Train Epoch: 2 [25936/33808 (77%)]\tLoss: 0.201881\n",
            "Train Epoch: 2 [26256/33808 (78%)]\tLoss: 0.131297\n",
            "Train Epoch: 2 [26576/33808 (79%)]\tLoss: 0.136517\n",
            "Train Epoch: 2 [26896/33808 (80%)]\tLoss: 0.451871\n",
            "Train Epoch: 2 [27216/33808 (80%)]\tLoss: 0.248489\n",
            "Train Epoch: 2 [27536/33808 (81%)]\tLoss: 0.256211\n",
            "Train Epoch: 2 [27856/33808 (82%)]\tLoss: 0.192236\n",
            "Train Epoch: 2 [28176/33808 (83%)]\tLoss: 0.266430\n",
            "Train Epoch: 2 [28496/33808 (84%)]\tLoss: 0.496906\n",
            "Train Epoch: 2 [28816/33808 (85%)]\tLoss: 0.091706\n",
            "Train Epoch: 2 [29136/33808 (86%)]\tLoss: 0.144601\n",
            "Train Epoch: 2 [29456/33808 (87%)]\tLoss: 0.204972\n",
            "Train Epoch: 2 [29776/33808 (88%)]\tLoss: 0.122840\n",
            "Train Epoch: 2 [30096/33808 (89%)]\tLoss: 0.306751\n",
            "Train Epoch: 2 [30416/33808 (90%)]\tLoss: 0.249571\n",
            "Train Epoch: 2 [30736/33808 (91%)]\tLoss: 0.383298\n",
            "Train Epoch: 2 [31056/33808 (92%)]\tLoss: 0.200733\n",
            "Train Epoch: 2 [31376/33808 (93%)]\tLoss: 0.569050\n",
            "Train Epoch: 2 [31696/33808 (94%)]\tLoss: 0.272301\n",
            "Train Epoch: 2 [32016/33808 (95%)]\tLoss: 0.149998\n",
            "Train Epoch: 2 [32336/33808 (96%)]\tLoss: 0.124945\n",
            "Train Epoch: 2 [32656/33808 (97%)]\tLoss: 0.371135\n",
            "Train Epoch: 2 [32976/33808 (97%)]\tLoss: 0.252097\n",
            "Train Epoch: 2 [33296/33808 (98%)]\tLoss: 0.279337\n",
            "Train Epoch: 2 [33616/33808 (99%)]\tLoss: 0.136504\n",
            "\n",
            " Average loss for Epoch 2: 0.24977885658812646 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqXKTFNTpTZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(myModel, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}