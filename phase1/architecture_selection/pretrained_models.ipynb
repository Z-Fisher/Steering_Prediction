{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steering_Z_Pretrained_Model_Sandbox.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjtpAf9kMxu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.utils.tensorboard as tb\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch import save\n",
        "from torch import load\n",
        "from os import path\n",
        "import torchvision.models as models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from torch import save\n",
        "from torch import load\n",
        "from os import path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo-FsC5hM615",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD81Hr71NIh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set current directory\n",
        "\n",
        "#%cd /content/drive/My Drive/Steering_Datasets_Rev1/CH2_002_Train/center\n",
        "# !unzip center_only.zip\n",
        "\n",
        "%cd /content/drive/My Drive/Colab Notebooks/CIS519 - Applied Machine Learning/Project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAMZsnkaSicC",
        "colab_type": "text"
      },
      "source": [
        "# DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvxQFobf9Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SteeringDataset(Dataset):\n",
        "    def __init__(self, image_path, data_transforms=None):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Hint: Use the python csv library to parse labels.csv\n",
        "        \"\"\"\n",
        "        # self.name_label_path = name_label_path\n",
        "        self.image_path = image_path\n",
        "        self.name = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(5,6))\n",
        "\n",
        "        self.labels = pd.read_csv(self.image_path + 'interpolated.csv', usecols=range(6,7))\n",
        "        self.center_data = pd.concat([self.name, self.labels], axis=1) #combine image name and label dataframes\n",
        "        self.center_data = self.center_data[self.center_data[\"filename\"].str.contains('center')] # only keep center image names and labels\n",
        "\n",
        "        self.name = pd.DataFrame(self.center_data[self.center_data.columns[0]]) # center images names\n",
        "        self.len = self.name.shape[0]\n",
        "        self.labels = pd.DataFrame(self.center_data[self.center_data.columns[1]]) # center image labels\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.ToTensor()])  \n",
        "                                #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        # bin angles into 100 classes\n",
        "\n",
        "        # classes = 100\n",
        "        # bins = np.linspace(-2.1,2,classes)\n",
        "        # labels = np.linspace(1,classes-1,classes-1).astype(int)\n",
        "        # self.labels = pd.cut(self.labels['angle'], bins = bins, labels = labels)\n",
        "\n",
        "        print(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        return a tuple: img, label\n",
        "        \"\"\"\n",
        "        # img = Image.open(str(self.name.iloc[idx, 0]))\n",
        "        img = Image.open(str(self.name.iloc[idx, 0])[7:]) # [7:] removes 'center/' before the image number since the zipped images just have numbers\n",
        "        img = img.crop((0, 240, 640, 480))  # crop image (remove above horizon). Original dimensions: 640,480. changing to 640,240 \n",
        "        img = img.resize((320,240)) #resize image\n",
        "        #img = img.resize((128,128)) #resize image\n",
        "\n",
        "        img = self.transform(img)\n",
        "        \n",
        "        label = self.labels.iloc[idx][0]\n",
        "        return img, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBI0gUIqZwmD",
        "colab_type": "text"
      },
      "source": [
        "# Save and Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpcbo4cZumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset_path, data_transforms, num_workers, batch_size, shuffle):\n",
        "    dataset = SteeringDataset(dataset_path, data_transforms)\n",
        "    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def train_test_split(args, dataset):\n",
        "    # Indices of Split\n",
        "    size = len(dataset)\n",
        "    indices = list(range(size))\n",
        "    train_size = int(args.test_fraction * size)\n",
        "    train_indices, test_indices = indices[train_size:], indices[:train_size]\n",
        "\n",
        "    # SubsetRandomSamplers\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # DataLoaders\n",
        "    train = DataLoader(dataset, batch_size=args.train_batch_size, sampler=train_sampler)\n",
        "    test = DataLoader(dataset, batch_size=args.test_batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def save_model(model, file_name):\n",
        "    #if isinstance(model, CNNClassifier):\n",
        "    return save(model.state_dict(), path.join(path.abspath(''), file_name))\n",
        "\n",
        "    #raise ValueError(\"model type '%s' not supported!\"%str(type(model)))\n",
        "\n",
        "def load_model(file_name):\n",
        "    r = CustomModel()\n",
        "    r.load_state_dict(load(path.join(path.abspath(''), file_name), map_location='cpu'))\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptEBrTX3YLl9",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1R0Fxl7YHMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationLoss(torch.nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Compute mean(-log(softmax(input)_label))\n",
        "        @input:  torch.Tensor((B,C)), where B = batch size, C = number of classes\n",
        "        @target: torch.Tensor((B,), dtype=torch.int64)\n",
        "        @return:  torch.Tensor((,))\n",
        "        Hint: Don't be too fancy, this is a one-liner\n",
        "        \"\"\"\n",
        "        m = nn.MSELoss()\n",
        "        return torch.sqrt(m(input.view(input.size(0)),target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbPyUoNYlSC",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5gGmDXZFhW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 5, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(5, 15, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(67860, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        @x: torch.Tensor((B,3,64,64))\n",
        "        @return: torch.Tensor((B,6))\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def vgg_model():\n",
        "    model = models.vgg11(pretrained=False)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True  # False: Freezes the weights of the pre-trained model\n",
        "    #model.classifier[3] = nn.Linear(4096, 4096)\n",
        "    model.classifier[6] = nn.Linear(4096, 1) # replace last fc layer\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet():\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # False: Freezes the weights of the pre-trained model\n",
        "    #model.fc = nn.Sequential(nn.Linear(2048, 2048),\n",
        "                             #nn.ReLU(),\n",
        "                             #nn.Linear(2048, 1))\n",
        "    model.fc = nn.Linear(2048, 1)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMRS_itGYoWS",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ulUussYd_f",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6F2zxvjYCVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "\n",
        "class Args(object):\n",
        "    def __init__(self, model):\n",
        "\n",
        "        # load_data\n",
        "        self.data_path = '/content/drive/My Drive/Colab Notebooks/CIS519 - Applied Machine Learning/Project/' #'/content/drive/My Drive/Steering_Datasets_Rev1/CH2_002_Train/'\n",
        "        self.transforms = None  # not in use\n",
        "        self.num_workers = 0  # not in use\n",
        "        self.train_batch_size = 64\n",
        "        self.test_batch_size = 1  # b/c batches aren't needed for testing\n",
        "        self.shuffle = False  # not in use\n",
        "\n",
        "        #train_test_split\n",
        "        self.test_fraction = 0.3\n",
        "\n",
        "        # Device\n",
        "        # self.device = torch.device(\"cuda\")\n",
        "\n",
        "        # train\n",
        "        self.learning_rate = 0.001 \n",
        "        self.momentum = 0.9 # not in use \n",
        "        self.epochs = 8\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = ClassificationLoss() #nn.CrossEntropyLoss() #RMSELoss().   CrossEntropyLoss: lable.long() conversion needed in train()\n",
        "\n",
        "        # test\n",
        "        self.correct_threshold = 0.1  # compared with prediction loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJMppDDJYqJT",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysIS-u6Fm_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args,model,train_data, test_data):\n",
        "    \"\"\"\n",
        "    Your code here\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize weights\n",
        "\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, mean=0, std=0.1)\n",
        "            torch.nn.init.constant_(m.bias, 0.1)\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            torch.nn.init.normal_(m.weight, mean=0, std=0.1)\n",
        "\n",
        "\n",
        "    # Model\n",
        "    model.train()\n",
        "    # model.to(args.device)\n",
        "    \n",
        "    # Optimizer & Criterion\n",
        "    optimizer = args.optimizer\n",
        "    criterion = args.criterion\n",
        "    # criterion.to(args.device)\n",
        "\n",
        "\n",
        "    # Book Keeping\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "    num_steps_per_epoch = len([i for i in range(len(train_data)) if i%10==0])\n",
        "\n",
        "\n",
        "    # Epoch Loop\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "\n",
        "        stepper = 0\n",
        "        model.train()\n",
        "        # Batch Loop\n",
        "        for batch_idx, (data, label) in enumerate(train_data):\n",
        "\n",
        "            # data, label = data.to(args.device), label.to(args.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Printing & Logging\n",
        "            if batch_idx % 1 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, (batch_idx+1) * len(data), len(train_data)*len(data),\n",
        "                  100. * batch_idx / len(train_data), loss.item()))                \n",
        "                train_losses.append(loss.item())\n",
        "                train_counter.append((batch_idx*64) + ((epoch-1)*len(train_data)))\n",
        "                stepper += 1\n",
        "\n",
        "            if batch_idx % 4 == 0:\n",
        "                print('Testing...')\n",
        "                valid_loss = test(args, model, test_data)\n",
        "                print(\"validation loss = \", valid_loss)\n",
        "\n",
        "        # check validation loss every epoch\n",
        "        \n",
        "        valid_loss = test(args, model, test_data)\n",
        "        print(\"validation loss = \", valid_loss)\n",
        "\n",
        "    #return model\n",
        "    save_model(model,'cnn_zach')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdXYzaoKYuuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, test_data):\n",
        "path\n",
        "    # Model\n",
        "    model.eval()\n",
        "    # model = model.to(args.device)\n",
        "\n",
        "    # Loss\n",
        "    criterion = args.criterion\n",
        "    # criterion.to(args.device)\n",
        "\n",
        "    # Book Keeping\n",
        "    test_loss = 0\n",
        "    correct = 0  # Since no classes: correct++ occurs when prediction is sufficiently close to target (within a threshold)\n",
        "    test_progress = 0\n",
        "\n",
        "    # Data Loop\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_data:\n",
        "\n",
        "            # Use Model\n",
        "            # data, target = data.to(args.device), target.to(args.device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target).item()\n",
        "            test_loss += loss\n",
        "\n",
        "            # #pred = output.argmax(dim=1, keepdim=True)  # don't need since no classes.\n",
        "            # if loss < args.correct_threshold:\n",
        "            #     correct += 1 #pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            # ## TODO: Build List of predictions, targets, and loss for each image so that data can be overlayed onto the images for the output video\n",
        "\n",
        "            # Print Progress\n",
        "            test_progress += 1\n",
        "            if test_progress % 100 == 0:\n",
        "              print('Images Processed: ', test_progress, ', Overall Progress: ', int(100*test_progress/len(test_data)*args.test_batch_size), '%')\n",
        "\n",
        "        test_loss /= len(test_data)\n",
        "\n",
        "        # print('\\nTest set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
        "        #       format(test_loss, correct, len(test_data.dataset), 100. * correct / len(test_data.dataset)))  \n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvRHvIT7FpSw",
        "colab_type": "code",
        "outputId": "b10c76de-6caa-4c5c-d4ce-4fd7ee5e3a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model = CNNClassifier()\n",
        "#model = vgg_model()    # predominantly pretrained deep nn. untrained output layer (4096->1)\n",
        "model = resnet()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qYyqhlJRBI",
        "colab_type": "code",
        "outputId": "3caa094a-ed54-470c-9b18-de25cef3c5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "# Parameters\n",
        "args = Args(model)\n",
        "\n",
        "# Load Data & Train/Test Split\n",
        "print('Loading Data...')\n",
        "train_data, test_data = train_test_split(args, SteeringDataset(args.data_path, args.transforms))\n",
        "\n",
        "print('Training...')\n",
        "train(args, model, train_data, test_data)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data...\n",
            "           angle\n",
            "2       0.001039\n",
            "5       0.003491\n",
            "8       0.003491\n",
            "11      0.005236\n",
            "14      0.006234\n",
            "...          ...\n",
            "101382  0.000000\n",
            "101385  0.000000\n",
            "101388  0.000000\n",
            "101391  0.000000\n",
            "101394  0.000000\n",
            "\n",
            "[33808 rows x 1 columns]\n",
            "Training...\n",
            "Train Epoch: 1 [64/23680 (0%)]\tLoss: 1.743885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-VgE-9E5QuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Model\n",
        "#test(args, model, test_data) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}